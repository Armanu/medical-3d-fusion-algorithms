{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for running IFCNN to fuse multiple types of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project page of IFCNN is https://github.com/uzeful/IFCNN.\n",
    "\n",
    "If you find this code is useful for your research, please consider to cite our paper.\n",
    "```\n",
    "@article{zhang2019IFCNN,\n",
    "  title={IFCNN: A General Image Fusion Framework Based on Convolutional Neural Network},\n",
    "  author={Zhang, Yu and Liu, Yu and Sun, Peng and Yan, Han and Zhao, Xiaolin and Zhang, Li},\n",
    "  journal={Information Fusion},\n",
    "  volume={54},\n",
    "  pages={99--118},\n",
    "  year={2020},\n",
    "  publisher={Elsevier}\n",
    "}\n",
    "```\n",
    "\n",
    "Detailed procedures to use IFCNN are introduced as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "from model import myIFCNN\n",
    "\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from utils.myTransforms import denorm, norms, detransformcv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the well-trained image fusion model (IFCNN-MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to C:\\Users\\Yuuuuu/.cache\\torch\\checkpoints\\resnet101-5d3b4d8f.pth\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "FloatProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\Software\\Anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                 \u001b[0mpbar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIProgress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# No total? Show info style bar with no progress tqdm status\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IProgress' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a8421a9be0a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# load pretrained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyIFCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuse_scheme\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfuse_scheme\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'snapshots/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\document\\Study\\Code\\其他代码\\IFCNN\\model.py\u001b[0m in \u001b[0;36mmyIFCNN\u001b[1;34m(fuse_scheme)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmyIFCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfuse_scheme\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;31m# pretrained resnet101\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m     \u001b[0mresnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet101\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m     \u001b[1;31m# our model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIFCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfuse_scheme\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfuse_scheme\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36mresnet101\u001b[1;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m     \"\"\"\n\u001b[0;32m    272\u001b[0m     return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n\u001b[1;32m--> 273\u001b[1;33m                    **kwargs)\n\u001b[0m\u001b[0;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\models\\resnet.py\u001b[0m in \u001b[0;36m_resnet\u001b[1;34m(arch, block, layers, pretrained, progress, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         state_dict = load_state_dict_from_url(model_urls[arch],\n\u001b[1;32m--> 223\u001b[1;33m                                               progress=progress)\n\u001b[0m\u001b[0;32m    224\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash)\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Downloading: \"{}\" to {}\\n'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[0mhash_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHASH_REGEX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcheck_hash\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[0mdownload_url_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;31m# Note: extractall() defaults to overwrite file if exists. No need to clean up beforehand.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\hub.py\u001b[0m in \u001b[0;36mdownload_url_to_file\u001b[1;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[0msha256\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m         with tqdm(total=file_size, disable=not progress,\n\u001b[1;32m--> 411\u001b[1;33m                   unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n\u001b[0m\u001b[0;32m    412\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m                 \u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8192\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0munit_scale\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         self.container = self.status_printer(\n\u001b[1;32m--> 209\u001b[1;33m             self.fp, total, self.desc, self.ncols)\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Software\\Anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[1;31m# #187 #451 #558\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             raise ImportError(\n\u001b[1;32m--> 104\u001b[1;33m                 \u001b[1;34m\"FloatProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                 \u001b[1;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[1;31mImportError\u001b[0m: FloatProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# we use fuse_scheme to choose the corresponding model, \n",
    "# choose 0 (IFCNN-MAX) for fusing multi-focus, infrare-visual and multi-modal medical images, 2 (IFCNN-MEAN) for fusing multi-exposure images\n",
    "fuse_scheme = 0\n",
    "if fuse_scheme == 0:\n",
    "    model_name = 'IFCNN-MAX'\n",
    "elif fuse_scheme == 1:\n",
    "    model_name = 'IFCNN-SUM'\n",
    "elif fuse_scheme == 2:\n",
    "    model_name = 'IFCNN-MEAN'\n",
    "else:\n",
    "    model_name = 'IFCNN-MAX'\n",
    "\n",
    "# load pretrained model\n",
    "model = myIFCNN(fuse_scheme=fuse_scheme)\n",
    "model.load_state_dict(torch.load('snapshots/'+ model_name + '.pth'))\n",
    "model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use IFCNN to respectively fuse CMF, IV and MD datasets\n",
    "Fusion images are saved in the 'results' folder under your current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time of CMF dataset: 1.23s\n",
      "Total processing time of IV dataset: 0.369s\n",
      "Total processing time of MD dataset: 0.0666s\n"
     ]
    }
   ],
   "source": [
    "from utils.myDatasets import ImagePair\n",
    "\n",
    "IV_filenames = ['Camp', 'Camp1', 'Dune', 'Gun', 'Navi', 'Kayak', 'Octec', 'Road', 'Road2', 'Steamboat', 'T2', 'T3', 'Trees4906', 'Trees4917', 'Window']\n",
    "MF_filenames = ['clock',  'lab', 'pepsi', 'book', 'flower', 'desk', 'seascape', 'temple', 'leopard', 'wine', 'balloon', 'calendar', 'corner', 'craft', 'leaf', 'newspaper', 'girl', 'grass', 'toy']\n",
    "\n",
    "datasets = ['CMF', 'IV', 'MD'] # Color MultiFocus, Infrared-Visual, MeDical image datasets\n",
    "datasets_num = [20, 15, 8]     # number of image sets in each dataset\n",
    "is_save = True                 # if you do not want to save images, then change its value to False\n",
    "\n",
    "for j in range(len(datasets)):\n",
    "    begin_time = time.time()\n",
    "    for ind in range(datasets_num[j]):\n",
    "        if j == 0:\n",
    "            # lytro-dataset: two images. Number: 20\n",
    "            dataset = datasets[j]      # Color Multifocus Images\n",
    "            is_gray = False            # Color (False) or Gray (True)\n",
    "            mean=[0.485, 0.456, 0.406] # normalization parameters\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "            \n",
    "            root = 'datasets/CMFDataset/'\n",
    "            filename = 'lytro-{:02}'.format(ind+1)\n",
    "            path1 = os.path.join('{0}-A.jpg'.format(root + filename))\n",
    "            path2 = os.path.join('{0}-B.jpg'.format(root + filename))\n",
    "        elif j == 1:\n",
    "            # infrare and visual image dataset. Number: 14\n",
    "            dataset = datasets[j]  # Infrared and Visual Images\n",
    "            is_gray = True         # Color (False) or Gray (True)\n",
    "            mean=[0, 0, 0]         # normalization parameters\n",
    "            std=[1, 1, 1]\n",
    "            \n",
    "            root = 'datasets/IVDataset/'\n",
    "            filename = IV_filenames[ind]\n",
    "            path1 = os.path.join(root, '{0}_Vis.png'.format(filename))\n",
    "            path2 = os.path.join(root, '{0}_IR.png'.format(filename))\n",
    "        elif j == 2:\n",
    "            # medical image dataset: CT (MR) and MR. Number: 8\n",
    "            dataset = datasets[j]  # Medical Image\n",
    "            is_gray = True         # Color (False) or Gray (True)\n",
    "            mean=[0, 0, 0]         # normalization parameters\n",
    "            std=[1, 1, 1]\n",
    "            \n",
    "            root = 'datasets/MDDataset/'\n",
    "            filename = 'c{:02}'.format(ind+1)\n",
    "            path1 = os.path.join('{0}_1.tif'.format(root + filename))\n",
    "            path2 = os.path.join('{0}_2.tif'.format(root + filename))\n",
    "\n",
    "        \n",
    "        # load source images\n",
    "        pair_loader = ImagePair(impath1=path1, impath2=path2, \n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=mean, std=std)\n",
    "                                  ]))\n",
    "        img1, img2 = pair_loader.get_pair()\n",
    "        img1.unsqueeze_(0)\n",
    "        img2.unsqueeze_(0)\n",
    "\n",
    "        # perform image fusion\n",
    "        with torch.no_grad():\n",
    "            res = model(Variable(img1.cuda()), Variable(img2.cuda()))\n",
    "            res = denorm(mean, std, res[0]).clamp(0, 1) * 255\n",
    "            res_img = res.cpu().data.numpy().astype('uint8')\n",
    "            img = res_img.transpose([1,2,0])\n",
    "\n",
    "        # save fused images\n",
    "        if is_save:\n",
    "            filename = model_name + '-' + dataset + '-' + filename\n",
    "            if is_gray:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "                img = Image.fromarray(img)\n",
    "                img.save('results/'+filename+'.png', format='PNG', compress_level=0)\n",
    "            else:\n",
    "                img = Image.fromarray(img)\n",
    "                img.save('results/'+filename+'.png', format='PNG', compress_level=0)\n",
    "\n",
    "    # when evluating time costs, remember to stop writing images by setting is_save = False\n",
    "    proc_time = time.time() - begin_time\n",
    "    print('Total processing time of {} dataset: {:.3}s'.format(datasets[j], proc_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use IFCNN to fuse triple multi-focus images in CMF dataset\n",
    "Fusion images are saved in the 'results' folder under your current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time of CMF3 dataset: 0.223s\n"
     ]
    }
   ],
   "source": [
    "from utils.myDatasets import ImageSequence\n",
    "\n",
    "dataset = 'CMF3'           # Triple Color MultiFocus\n",
    "is_save = True             # Whether to save the results\n",
    "is_gray = False            # Color (False) or Gray (True)\n",
    "is_folder=False            # one parameter in ImageSequence\n",
    "mean=[0.485, 0.456, 0.406] # Color (False) or Gray (True)\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "begin_time = time.time()\n",
    "for ind in range(4):\n",
    "    # load the sequential source images\n",
    "    root = 'datasets/CMFDataset/Triple Series/'\n",
    "    filename = 'lytro-{:02}'.format(ind+1)\n",
    "    paths = []\n",
    "    paths.append(os.path.join('{0}-A.jpg'.format(root + filename)))\n",
    "    paths.append(os.path.join('{0}-B.jpg'.format(root + filename)))\n",
    "    paths.append(os.path.join('{0}-C.jpg'.format(root + filename)))\n",
    "    filename = model_name + '-' + dataset + '-' + 'lytro-{:02}'.format(ind+1)\n",
    "\n",
    "    seq_loader = ImageSequence(is_folder, 'RGB', transforms.Compose([\n",
    "                                  transforms.ToTensor(),\n",
    "                                  transforms.Normalize(mean=mean, std=std),\n",
    "                                ]), *paths)\n",
    "    imgs = seq_loader.get_imseq()\n",
    "    \n",
    "    # perform image fusion\n",
    "    with torch.no_grad():\n",
    "        vimgs = []\n",
    "        for idx, img in enumerate(imgs):\n",
    "            img.unsqueeze_(0)\n",
    "            vimgs.append(Variable(img.cuda()))\n",
    "        vres = model(*vimgs)\n",
    "        res = denorm(mean, std, vres[0]).clamp(0, 1) * 255\n",
    "        res_img = res.cpu().data.numpy().astype('uint8')\n",
    "        img = Image.fromarray(res_img.transpose([1,2,0]))\n",
    "    \n",
    "    # save the fused image\n",
    "    if is_save:\n",
    "        if is_gray:\n",
    "            img.convert('L').save('results/'+filename+'.png', format='PNG', compress_level=0)\n",
    "        else:\n",
    "            img.save('results/'+filename+'.png', format='PNG', compress_level=0)\n",
    "            \n",
    "# when evluating time costs, remember to stop writing images by setting is_save = False\n",
    "proc_time = time.time() - begin_time\n",
    "print('Total processing time of {} dataset: {:.3}s'.format(dataset ,proc_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load the well-trained image fusion model (IFCNN-MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use fuse_scheme to choose the corresponding model, \n",
    "# choose 0 (IFCNN-MAX) for fusing multi-focus, infrare-visual and multi-modal medical images, 2 (IFCNN-MEAN) for fusing multi-exposure images\n",
    "fuse_scheme = 2\n",
    "if fuse_scheme == 0:\n",
    "    model_name = 'IFCNN-MAX'\n",
    "elif fuse_scheme == 1:\n",
    "    model_name = 'IFCNN-SUM'\n",
    "elif fuse_scheme == 2:\n",
    "    model_name = 'IFCNN-MEAN'\n",
    "else:\n",
    "    model_name = 'IFCNN-MAX'\n",
    "\n",
    "# load pretrained model\n",
    "model = myIFCNN(fuse_scheme=fuse_scheme)\n",
    "model.load_state_dict(torch.load('snapshots/'+ model_name + '.pth'))\n",
    "model.eval()\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use IFCNN to fuse various number of multi-exposure images in ME Dataset\n",
    "Fusion images are saved in the 'results' folder under your current folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total processing time of ME dataset: 0.32s\n"
     ]
    }
   ],
   "source": [
    "from utils.myDatasets import ImageSequence\n",
    "\n",
    "dataset = 'ME'\n",
    "is_save = True\n",
    "is_gray = False\n",
    "is_folder = True\n",
    "toggle = True\n",
    "is_save_Y = False\n",
    "mean=[0, 0, 0]\n",
    "std=[1, 1, 1]\n",
    "begin_time = time.time()\n",
    "root = 'datasets/MEDataset/'\n",
    "\n",
    "for subdir, dirs, files in os.walk(root):\n",
    "    if toggle:\n",
    "        toggle = False\n",
    "    else:\n",
    "       # Load the sequential images in each subfolder\n",
    "        paths = [subdir]\n",
    "        seq_loader = ImageSequence(is_folder, 'YCbCr', transforms.Compose([\n",
    "                                      transforms.ToTensor()]), *paths)\n",
    "        imgs = seq_loader.get_imseq()\n",
    "\n",
    "        # separate the image channels\n",
    "        NUM = len(imgs)\n",
    "        c, h, w = imgs[0].size()\n",
    "        Cbs = torch.zeros(NUM, h, w)\n",
    "        Crs = torch.zeros(NUM, h, w)\n",
    "        Ys = []\n",
    "        for idx, img in enumerate(imgs):\n",
    "                #print(img)\n",
    "                Cbs[idx,:,:] = img[1]\n",
    "                Crs[idx,:,:] = img[2]\n",
    "                Ys.append(img[0].unsqueeze_(0).unsqueeze_(0).repeat(1,3,1,1)) #Y\n",
    "\n",
    "        # Fuse the color channels (Cb and Cr) of the image sequence\n",
    "        Cbs *= 255\n",
    "        Crs *= 255\n",
    "        Cb128 = abs(Cbs - 128);\n",
    "        Cr128 = abs(Crs - 128);\n",
    "        CbNew = sum((Cbs * Cb128) / (sum(Cb128).repeat(NUM, 1, 1)));\n",
    "        CrNew = sum((Crs * Cr128) / (sum(Cr128).repeat(NUM, 1, 1)));\n",
    "        CbNew[torch.isnan(CbNew)] = 128\n",
    "        CrNew[torch.isnan(CrNew)] = 128\n",
    "\n",
    "        # Fuse the Y channel of the image sequence\n",
    "        imgs = norms(mean, std, *Ys) # normalize the Y channels\n",
    "        with torch.no_grad():\n",
    "            vimgs = []\n",
    "            for idx, img in enumerate(imgs):\n",
    "                vimgs.append(Variable(img.cuda()))\n",
    "            vres = model(*vimgs)\n",
    "\n",
    "        # Enhance the Y channel using CLAHE\n",
    "        img = detransformcv2(vres[0], mean, std)                    # denormalize the fused Y channel\n",
    "        y = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)                   # generate the single y channel\n",
    "        \n",
    "        y = y / 255                                                 # initial enhancement\n",
    "        y = y * 235 + (1-y) * 16;\n",
    "        y = y.astype('uint8')\n",
    "\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))  # clahe enhancement\n",
    "        cy = clahe.apply(y)\n",
    "        \n",
    "        # Merge the YCbCr channels back and covert to RGB color space\n",
    "        cyrb = np.zeros([h,w,c]).astype('uint8')\n",
    "        cyrb[:,:,0] = cy\n",
    "        cyrb[:,:,1] = CrNew\n",
    "        cyrb[:,:,2] = CbNew\n",
    "        rgb = cv2.cvtColor(cyrb, cv2.COLOR_YCrCb2RGB)\n",
    "        \n",
    "        \n",
    "        # Save the fused image\n",
    "        img = Image.fromarray(rgb)\n",
    "        filename = subdir.split('/')[-1]\n",
    "        filename = model_name + '-' + dataset + '-' + filename  # y channels are fused by IFCNN, cr and cb are weighted fused\n",
    "        \n",
    "        if is_save:\n",
    "            if is_gray:\n",
    "                img.convert('L').save('results/'+filename+'.png', format='PNG', compress_level=0)\n",
    "            else:\n",
    "                img.save('results/'+filename+'.png', format='PNG', compress_level=0)\n",
    "\n",
    "# when evluating time costs, remember to stop writing images by setting is_save = False\n",
    "proc_time = time.time() - begin_time\n",
    "print('Total processing time of {} dataset: {:.3}s'.format(dataset, proc_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
